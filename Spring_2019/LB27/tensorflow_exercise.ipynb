{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this demo comes from the MIT Deep Learning course: https://deeplearning.mit.edu/\n",
    "\n",
    "as well as Google's Tensorboard tutorial: https://www.tensorflow.org/tensorboard/r1/overview/\n",
    "\n",
    "the high resolution digits were generated this way: http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required and Accessory Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "# Logging for Tensorboard\n",
    "from time import time\n",
    "from tensorflow.keras.callbacks import TensorBoard # If you want to use tb, make sure to have a log directory\n",
    "\n",
    "# Commonly used modules\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Images, plots, display, and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2 # installed as opencv\n",
    "import IPython\n",
    "from six.moves import urllib\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data and Reshape\n",
    "The data we're using is the MNIST digit dataset. It's a set of grayscale images so there should be only 1 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# reshape images to specify that it's a single channel\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "Neural networks work best with the data normalized to a range between 0 and 1.\n",
    "Make sure to process the training and testing data in exactly the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(imgs): # works for either a single image or multiple images\n",
    "    sample_img = imgs if len(imgs.shape) == 2 else imgs[0]\n",
    "    assert sample_img.shape in [(28, 28, 1), (28, 28)], sample_img.shape # make sure images are 28x28 and single-channel (grayscale)\n",
    "    return ??? # normalize to [0,1] here\n",
    "\n",
    "train_images = ???\n",
    "test_images = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we didn't break anything and display some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can build our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I recommend a CNN\n",
    "model = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit our model, we need to add a few more things:\n",
    "* An optimizer\n",
    "* A loss function\n",
    "* Metrics to measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard!\n",
    "Let's collect some data so we can see what Tensorboard can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"./logs/{}\".format(time()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what we get, open a terminal and run the following:\n",
    "```\n",
    "tensorboard --logdir=./logs/\n",
    "```\n",
    "where logdir is the relative path to your logs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Let's Train Our Model\n",
    "We need to feed in our data to the model we've constructed and decide how many epochs (loops through the training set) we want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/jpa84/Documents/Python Scripts/Lunch_Bytes\n"
     ]
    }
   ],
   "source": [
    "# This is just me making sure that my logs go to the right place\n",
    "os.chdir('C:/Users/jpa84/Documents/Python Scripts/Lunch_Bytes/')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For the model that I made, each epoch took about 80 seconds to complete. I wouldn't recommend more than two or three epochs (to keep this tutorial moving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 77s 1ms/sample - loss: 0.0340 - acc: 0.9893\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0303 - acc: 0.9906\n"
     ]
    }
   ],
   "source": [
    "training = model.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Evaluate Our Model\n",
    "To see if our model is any good, we need to evaluate it on the test images. This will tell us how well our model generalizes to other MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_images.shape) # check that the dimensions of the test data match the training data in case of any errors\n",
    "test_loss, test_acc = model.???\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc*100.0, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! the model is about 99% accurate (in my test runthrough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What About Digits Not From MNIST?\n",
    "Test data doesn't represent data \"from the wild\" exactly, so let's read in some high resolution images and see how the model performs.\n",
    "This part of the notebook doesn't involve much TensorFlow so I've left the code intact.\n",
    "\n",
    "This is meant to be an example of a more interesting application of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dream_path = 'images/mnist_dream.mp4'\n",
    "mnist_prediction_path = 'images/mnist_dream_predicted.mp4'\n",
    "\n",
    "# download the video if you haven't\n",
    "if not os.path.isfile(mnist_dream_path): \n",
    "    print('downloading the sample video...')\n",
    "    vid_url = 'https://github.com/lexfridman/mit-deep-learning/raw/master/tutorial_deep_learning_basics' + '/' + mnist_dream_path\n",
    "    \n",
    "    mnist_dream_path = urllib.request.urlretrieve(vid_url)[0]\n",
    "                                                                                                  \n",
    "def cv2_imshow(img):\n",
    "    ret = cv2.imencode('.png', img)[1].tobytes() \n",
    "    img_ip = IPython.display.Image(data=ret)\n",
    "    IPython.display.display(img_ip)\n",
    "\n",
    "cap = cv2.VideoCapture(mnist_dream_path) \n",
    "vw = None\n",
    "frame = -1 # counter for debugging (mostly), 0-indexed\n",
    "\n",
    "# go through all the frames and run our classifier on the high res MNIST images as they morph from number to number\n",
    "while True: # should 481 frames\n",
    "    frame += 1\n",
    "    ret, img = cap.read()\n",
    "    if not ret: break\n",
    "               \n",
    "    assert img.shape[0] == img.shape[1] # should be a square\n",
    "    if img.shape[0] != 720:\n",
    "        img = cv2.resize(img, (720, 720))\n",
    "       \n",
    "    #preprocess the image for prediction\n",
    "    img_proc = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_proc = cv2.resize(img_proc, (28, 28))\n",
    "    img_proc = preprocess_images(img_proc)\n",
    "    img_proc = 1 - img_proc # inverse since training dataset is white text with black background\n",
    "\n",
    "    net_in = np.expand_dims(img_proc, axis=0) # expand dimension to specify batch size of 1\n",
    "    net_in = np.expand_dims(net_in, axis=3) # expand dimension to specify number of channels\n",
    "    \n",
    "    preds = model.predict(net_in)[0]\n",
    "    guess = np.argmax(preds)\n",
    "    perc = np.rint(preds * 100).astype(int)\n",
    "    \n",
    "    img = 255 - img\n",
    "    pad_color = 0\n",
    "    img = np.pad(img, ((0,0), (0,1280-720), (0,0)), mode='constant', constant_values=(pad_color))  \n",
    "    \n",
    "    line_type = cv2.LINE_AA\n",
    "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1.3        \n",
    "    thickness = 2\n",
    "    x, y = 740, 60\n",
    "    color = (255, 255, 255)\n",
    "    \n",
    "    text = \"Neural Network Output:\"\n",
    "    cv2.putText(img, text=text, org=(x, y), fontScale=font_scale, fontFace=font_face, thickness=thickness,\n",
    "                    color=color, lineType=line_type)\n",
    "    \n",
    "    text = \"Input:\"\n",
    "    cv2.putText(img, text=text, org=(30, y), fontScale=font_scale, fontFace=font_face, thickness=thickness,\n",
    "                    color=color, lineType=line_type)   \n",
    "        \n",
    "    y = 130\n",
    "    for i, p in enumerate(perc):\n",
    "        if i == guess: color = (255, 218, 158)\n",
    "        else: color = (100, 100, 100)\n",
    "            \n",
    "        rect_width = 0\n",
    "        if p > 0: rect_width = int(p * 3.3)\n",
    "        \n",
    "        rect_start = 180\n",
    "        cv2.rectangle(img, (x+rect_start, y-5), (x+rect_start+rect_width, y-20), color, -1)\n",
    "\n",
    "        text = '{}: {:>3}%'.format(i, int(p))\n",
    "        cv2.putText(img, text=text, org=(x, y), fontScale=font_scale, fontFace=font_face, thickness=thickness,\n",
    "                    color=color, lineType=line_type)\n",
    "        y += 60\n",
    "    \n",
    "    # if you don't want to save the output as a video, set this to False\n",
    "    save_video = True\n",
    "    \n",
    "    if save_video:\n",
    "        if vw is None:\n",
    "            codec = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "            vid_width_height = img.shape[1], img.shape[0]\n",
    "            vw = cv2.VideoWriter(mnist_prediction_path, codec, 30, vid_width_height)\n",
    "        # 15 fps above doesn't work robustly so we right frame twice at 30 fps\n",
    "        vw.write(img)\n",
    "        vw.write(img)\n",
    "    \n",
    "    # scale down image for display\n",
    "    img_disp = cv2.resize(img, (0,0), fx=0.5, fy=0.5)\n",
    "    cv2_imshow(img_disp)\n",
    "    IPython.display.clear_output(wait=True)\n",
    "        \n",
    "cap.release()\n",
    "if vw is not None:\n",
    "    vw.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's About It!\n",
    "\n",
    "That's all it takes to create a convolutional neural network using TensorFlow and Keras. It's a pretty simple process, but larger and more complicated networks can be a bit more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for Listening/Participating!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
